---
title: "Project 3"
author: "Jennifer Le"
date: "5/4/18"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr)
library(ggplot2)
library(dplyr)
library(broom)
```


# Part 1: Regression analysis of Gapminder data

```{r}
library(gapminder)
data(gapminder)

gapminder
```


Exercise 1: Make a scatter plot of life expectancy across time.

```{r exercise1}
gapminder %>%
  ggplot(mapping=aes(x=year, y=lifeExp)) +
  geom_point() + labs(x="Year", y="Life Expectancy", title="Life Expectancy Across Time")
```

Question 1: Is there a general trend (e.g., increasing or decreasing) for life expectancy across time? Is this trend linear?

According to the scatterplot, there is a general increasing trend that is linear for life expectancy across time.

A slightly different way of making the same plot is looking at the distribution of life expectancy across countries as it changes over time:

```{r}
library(tidyverse)
library(ggplot2)

gapminder %>%
  ggplot(aes(x=factor(year), y=lifeExp)) +
    geom_violin() +
    labs(x="Year", y="Life Expectancy", title="Life Expectancy Over Time")
```

Question 2: How would you describe the distribution of life expectancy across countries for individual years? Is it skewed, or not? Unimodal or not? Symmetric around its center?

The distribution of life expectancy for years for all the years plotted are slightly skewed. All of the violin plots are unimodal except for years 1962-1977. None of the distributions are symmetric around the center. 

Question 3: Suppose I fit a linear regression model of life expectancy vs. year (treating it as a continuous variable), and test for a relationship between year and life expectancy, will you reject the null hypothesis of no relationship? (do this without fitting the model yet. I am testing your intuition.)

I would reject the null hypothesis because from the violin plot, we can see that there's a steady increase in life expectancy over the years, which means that there must be a relationship between year and life expectancy. 

Question 4: What would a violin plot of residuals from the linear model in Question 3 vs. year look like? (Again, don’t do the analysis yet, answer this intuitively)

The plot would probably look like the violin plot in Question 1 but instead with a decreasing trend over time. Since the linear regression model is accurate enough, the residuals will get closer to zero. 

Question 5: According to the assumptions of the linear regression model, what should that violin plot look like?

I think the violin plot would be unimodal, not skewed, and centered around the middle, which is zero. 

Exercise 2: Fit a linear regression model using the lm function for life expectancy vs. year (as a continuous variable). Use the broom::tidy to look at the resulting model.

```{r exercise2}
gapminder %>%
  ggplot(aes(x=year, y=lifeExp)) +
  geom_point() +
  geom_smooth(method=lm) +
  labs(x="Year", y="Life Expectancy", title="Life Expectancy Over Time")

gapminderLM <- lm(lifeExp~year, data=gapminder)
gapminderLM %>% broom::tidy()
```

Question 6: On average, by how much does life expectancy increase every year around the world?

On average, life expectancy increases by 0.3259038 years each year.

Question 7: Do you reject the null hypothesis of no relationship between year and life expectancy? Why?

Yes I would reject the null hypothesis because the p-value 7.546795e-80 is very close to 0 and is smaller than 0.05.

Exercise 3: Make a violin plot of residuals vs. year for the linear model from Exercise 2 (use the broom::augment function).

```{r exercise3}
gapminderAug <- gapminderLM %>%
  augment()
gapminderAug %>%
  head(10)

gapminderAug %>%
  ggplot(mapping=aes(x=factor(year), y=.resid)) + 
  geom_violin() + labs(x="Year", y="Residual", title="Residuals vs. Year")
```

Question 8: Does the plot of Exercise 3 match your expectations (as you answered Question 4)?

Yes, the plot in Exercise 3 matches what I answered in Question 4 because the plots show a decreasing trend over time with the residuals being close to zero.

Exercise 4: Make a boxplot (or violin plot) of model residuals vs. continent.

```{r exercise4}
gapminderAug$continent <- gapminder$continent

gapminderAug %>%
  ggplot(mapping=aes(x=continent, y=.resid)) + 
  geom_boxplot() + labs(x="Continent", y="Residual", title="Residuals vs. Continent")
```

Question 9: Is there a dependence between model residual and continent? If so, what would that suggest when performing a regression analysis of life expectancy across time?

Yes, there's a dependence between model residual and continent. This would suggest that continent would have an effect on life expectancy and that there would be different regression lines for each continent.

Exercise 5: Use geom_smooth(method=lm) in ggplot as part of a scatter plot of life expectancy vs. year, grouped by continent (e.g., using the color aesthetic mapping).

```{r exercise5}
gapminder %>%
  ggplot(mapping=aes(x=year, y=lifeExp, color=continent)) + 
  geom_point() + labs(x="Year", y="Life Expectancy", title="Life Expectancy Across Time") +
  geom_smooth(method=lm)
```

Question 10: Based on this plot, should your regression model include an interaction term for continent and year? Why?

Yes, my regression model should include an interaction term for continent and year because the regression line for each continent is different from the others, so we can see that continent has an effect on life expectancy. 

Exercise 6: Fit a linear regression model for life expectancy including a term for an interaction between continent and year. Use the broom::tidy function to show the resulting model.

```{r exercise6}
gapminderContYear <- lm(lifeExp~continent*year, data=gapminder)
gapminderContYear %>% tidy()
```

Question 11: Are all parameters in the model significantly different from zero? If not, which are not significantly different from zero?

No, not all of the parameters in the model are significantly different from zero. The only two that are significantly different from zero are continentOceania and continentOceania:year. The rest of the parameters are very close to zero. 

Question 12: On average, by how much does life expectancy increase each year for each continent? (Provide code to answer this question by extracting relevant estimates from model fit)

```{r question12}
gapminderContYear %>% tidy()

africa <- 0.28952926
africa
americas <- 0.28952926 + 0.07812167
americas
asia <- 0.28952926 + 0.16359314
asia
europe <- 0.28952926 - 0.06759712
europe
oceania <- 0.28952926 - 0.07925689
oceania
```
For Africa, life expectancy increases by 0.28952926 years each year. For the Americas, life expectancy increases by 0.3676509 years each year. For Asia, life expectancy increases by 0.4531224 years each year. For Europe, life expectancy decreases by 0.2219321 years each year. For Oceania, life expectancy decreases 0.2102724 years each year. 

Exercise 7: Use the anova function to perform an F-test that compares how well two models fit your data: (a) the linear regression models from Exercise 2 (only including year as a covariate) and (b) Exercise 6 (including interaction between year and continent).

```{r exercise7}
anova(gapminderLM)
anova(gapminderContYear)
```

Question 13: Is the interaction model significantly better than the year-only model? Why?

An F-value that is larger than 1 indicates that there is a relationship between the response and the predictor. An F-value that is close to 1 indicates that there is no relationship between the two. The F-value for the year-only model is 398.6, but the F-values for the interaction model are 675.812 and 1046.028, which are much larger. Also, the p-values for both models are very close to zero so this means that the interaction is not significantly better, but only a little better than the year-only model.


Exercise 8: Make a residuals vs. year violin plot for the interaction model. Comment on how well it matches assumptions of the linear regression model. Do the same for a residuals vs. fitted values model. (You should use the broom::augment function).

```{r exercise8a}
gapminderContYear %>%
  augment() %>%
  ggplot(aes(x=factor(year), y=.resid)) + geom_violin() +
  labs(x="Year", y="Residual", title="Residuals vs. Year")
```

The residuals for the "Residuals vs. Year" plot are very close to zero. 

```{r exercise8b}
gapminderContYear %>%
  augment() %>%
  ggplot(aes(x=.fitted, y=.resid)) + geom_point() +
  labs(x="Fitted", y="Residual", title="Residuals vs. Fitted")
```

The residuals for the "Residuals vs. Fitted" plot are also very close to zero. 



# Part 2: Classification

### Preparing data
```{r, include=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
theme_set(theme_bw())
```

```{r}
csv_file <- "/Users/jJ/Documents/CMSC320/Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford
```

```{r}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")
```

### The prediction task

The prediction task we are going to answer is: 
Can we predict if mortgage affordability will increase or decrease a year from now?

Specifically, we will do this for the last observation in the dataset (quarter 4 (Q4) of 2017). To create the outcome we will predict we will compare affordability for Q4 of 2017 and to Q4 of 2016 and label it as up or down depending on the sign of the this difference. Let’s create the outcome we want to predict (again, copy this bit of code to your submission):

```{r}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```

Now, you have a dataframe with outcomes (labels) for each county in the dataset.

The goal is then given predictors Xi for county i, build a classifier for outcome Gi∈{𝚞𝚙,𝚍𝚘𝚠𝚗}.

For your classifiers you should use data up to 2016.
```{r}
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```


### My project

The question I chose to address is: 
Does standardizing affordability for each region affect prediction performance? Compare standardized to non-standardized affordability.

Standardized variables are values that are rescaled into a common format to make it easier to compare scores that were measured on different scales. These variables have been standardized to have a mean of 0 and a standard deviation of 1. A value's standardized form indicates its difference from the mean of the original variable in number of standard deviations. I wanted to test to see if standardizing affordability for each region has any effect on the prediction performance versus non-standardized affordability. In order to carry out this experiment of feature representation and preprocessing, I used two separate random forests each with unstandardized or standardized data. I kept the amount of trees in each forest the same so there would not be any skewed data.


```{r, include=FALSE, message=FALSE}
library(rfUtilities)
library(ISLR)
library(cvTools)
library(tree)
library(caret)
library(ROCR)
```

Unstandardized data
```{r, message=FALSE}
library(randomForest)

standardized_df <- predictor_df %>%
  filter(year(time) %in% 2014:2016) %>%
  group_by(RegionID) %>%
  mutate(mean_aff = mean(affordability)) %>%
  mutate(sd_aff = sd(affordability)) %>%
  mutate(z_aff = (affordability - mean_aff) / sd_aff) %>%
  ungroup()
standardized_df

wide_df <- standardized_df %>%
  select(RegionID, time, affordability) %>%
  tidyr::spread(time, affordability)
wide_df

matrix_1 <- wide_df %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-1]

matrix_2 <- wide_df %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-ncol(.)]

diff_df <- (matrix_1 - matrix_2) %>%
  magrittr::set_colnames(NULL) %>%
  as_data_frame() %>%
  mutate(RegionID = wide_df$RegionID)

final_df <- diff_df %>%
  inner_join(outcome_df %>% select(RegionID, Direction), by="RegionID") %>%
  mutate(Direction=factor(Direction, levels=c("down", "up")))
final_df

set.seed(1234)
test_random_forest_df <- final_df %>%
  group_by(Direction) %>%
  sample_frac(.2) %>%
  ungroup()

train_random_forest_df <- final_df %>%
  anti_join(test_random_forest_df, by="RegionID")

rf <- randomForest(Direction~., data=train_random_forest_df %>% select(-RegionID))
rf

test_predictions <- predict(rf, newdata=test_random_forest_df %>% select(-RegionID))

table(pred=test_predictions, observed=test_random_forest_df$Direction)

errorRate <- (0+3) / (3+0+3+10)
errorRate
```


Standardized data
```{r}
wide_df2 <- standardized_df %>%
  select(RegionID, time, z_aff) %>%
  tidyr::spread(time, z_aff)
wide_df2

matrix_1a <- wide_df2 %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-1]

matrix_2a <- wide_df2 %>%
  select(-RegionID) %>%
  as.matrix() %>%
  .[,-ncol(.)]

diff_df2 <- (matrix_1a - matrix_2a) %>%
  magrittr::set_colnames(NULL) %>%
  as_data_frame() %>%
  mutate(RegionID = wide_df2$RegionID)

final_df2 <- diff_df2 %>%
  inner_join(outcome_df %>% select(RegionID, Direction), by="RegionID") %>%
  mutate(Direction=factor(Direction, levels=c("down", "up")))
final_df2

set.seed(1234)
test_random_forest_df2 <- final_df2 %>%
  group_by(Direction) %>%
  sample_frac(.2) %>%
  ungroup()

train_random_forest_df2 <- final_df2 %>%
  anti_join(test_random_forest_df2, by="RegionID")

rf2 <- randomForest(Direction~., data=train_random_forest_df2 %>% select(-RegionID))
rf2

test_predictions2 <- predict(rf2, newdata=test_random_forest_df2 %>% select(-RegionID))

table(pred=test_predictions2, observed=test_random_forest_df2$Direction)

errorRate2 <- (1+4) / (2+1+4+9)
errorRate2
```

#### Cross Validation
```{r}
set.seed(1234)

# create the cross-validation partition
result_df <- createFolds(final_df$Direction, k=10) %>%
  # fit models and gather results
  purrr::imap(function(test_indices, fold_number) {
    # split into train and test for the fold
    # using unstandardized data
    train_df <- final_df %>%
      select(-RegionID) %>%
      slice(-test_indices)

    test_df <- final_df %>%
      select(-RegionID) %>%
      slice(test_indices)
    
    # using standardized data
    train_df2 <- final_df2 %>%
      select(-RegionID) %>%
      slice(-test_indices)
    
    test_df2 <- final_df2 %>%
      select(-RegionID) %>%
      slice(test_indices)
  
    # fit the two models
    rf3 <- randomForest(Direction~., data=train_df, ntree=500)
    rf4 <- randomForest(Direction~., data=train_df2, ntree=500)
    
    # gather results
    test_df %>%
      select(observed_label = Direction) %>%
      mutate(fold=fold_number) %>%
      mutate(prob_positive_rf1 = predict(rf3, newdata=test_df, type="prob")[,"up"]) %>%
      # add predicted labels for rf1 using a 0.5 probability cutoff
      mutate(predicted_label_rf1 = ifelse(prob_positive_rf1 > 0.5, "up", "down")) %>%
      mutate(prob_positive_rf2 = predict(rf4, newdata=test_df2, type="prob")[, "up"]) %>%
      # add predicted labels for rf2 using a 0.5 probability cutoff
      mutate(predicted_label_rf2 = ifelse(prob_positive_rf2 > 0.5, "up", "down"))
}) %>%
  # combine the five result data frames into one
  purrr::reduce(bind_rows)
result_df
```

```{r}
result_df %>%
  mutate(error_rf1 = observed_label != predicted_label_rf1,
         error_rf2 = observed_label != predicted_label_rf2) %>%
  group_by(fold) %>%
  summarize(unstandardized_rf = mean(error_rf1), standardized_rf = mean(error_rf2)) %>%
  tidyr::gather(model, error, -fold) %>%
  lm(error~model, data=.) %>%
  broom::tidy()
```


#### AUROC Plot
```{r}
# create a list of true observed labels 
labels <- split(result_df$observed_label, result_df$fold)

# now create a list of predictions for the first RF and pass it to the ROCR::prediction function
predictions_rf1 <- split(result_df$prob_positive_rf1, result_df$fold) %>% prediction(labels)

# do the same for the second RF
predictions_rf2 <- split(result_df$prob_positive_rf2, result_df$fold) %>% prediction(labels)

# compute average AUC for the first RF
mean_auc_rf1 <- predictions_rf1 %>%
  performance(measure="auc") %>%
  # I know, this line is ugly, but that's how it is
  slot("y.values") %>% unlist() %>% 
  mean()

# compute average AUC for the second RF
mean_auc_rf2 <- predictions_rf2 %>%
  performance(measure="auc") %>%
  slot("y.values") %>% unlist() %>% 
  mean()

# plot the ROC curve for the first RF
predictions_rf1 %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="orange", lwd=2)

# plot the ROC curve for the second RF
predictions_rf2 %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="blue", lwd=2, add=TRUE)

# add a legend to the plot
legend("bottomright",
       legend=paste(c("unstandardized", "standardized"), "rf, AUC:", round(c(mean_auc_rf1, mean_auc_rf2), digits=3)),
       col=c("orange", "blue"))
```

### Interpretation and discussion of my experiment results

The error rate for unstandardized affordability calculated from the confusion matrix was 18.75%. The error rate for standardized affordability was 31.25%. The lower error rate for unstandardized affordability shows that it is slightly better than standardized affordability in predicting whether mortgage affordability would increase or decrease a year from now. The AUROC plot also shows a similar comparison as the error rates. From the plot, we see that the AUROC score for unstandardized affordability is 0.664 and for standardized affordability the score is 0.569. The higher AUROC score for unstandardized indicates that the rate of true positive classifications is greater than the rate of false positive classifications. In both cases, we can see that not standardizing affordability is better than standardizing in its prediction performance, but we need to determine if there is a significant difference. After doing 10-fold cross validation, we get a p-value of 0.7419572, which is greater than 0.05. Therefore, we must accept the null hypothesis that there is not a significant difference between unstandardized and standardized affordability.


